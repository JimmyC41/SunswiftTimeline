{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64945ba0-7256-4c1f-818a-c12b3729d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "from datetime import datetime, timedelta, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f518caf-cc23-4f6f-8141-0b21d1fe8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- constants ---\n",
    "RUN_DATE = \"10_8_25\"\n",
    "\n",
    "# parent directory of current script\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"Results\", RUN_DATE)\n",
    "\n",
    "# ensure directory exists\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# output filename\n",
    "outname = os.path.join(RESULTS_DIR, f\"timeline_{uuid.uuid4().hex[:8]}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0c3956a-c5db-45d3-864e-53de37084db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Load cleaned Excel\n",
    "# -----------------------------\n",
    "WORKBOOK_PATH = Path(f\"../../data/SR8_{RUN_DATE}_v2.xlsx\")\n",
    "df = pd.read_excel(WORKBOOK_PATH, sheet_name=0)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Assign each assembly a unique integer ID\n",
    "# -----------------------------\n",
    "unique_assys = df['Assembly'].unique().tolist()\n",
    "assy_to_id = {name: idx for idx, name in enumerate(unique_assys, start=1)}\n",
    "df['ID'] = df['Assembly'].map(assy_to_id)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Parse “Predecessors” and “Successors” into lists of integer IDs\n",
    "# -----------------------------\n",
    "def names_to_ids(cell):\n",
    "    if pd.isna(cell) or str(cell).strip() in ('', 'N/A', '-'):\n",
    "        return []\n",
    "    names = [x.strip() for x in str(cell).split(',') if x.strip()]\n",
    "    return [assy_to_id[n] for n in names if n in assy_to_id]\n",
    "\n",
    "df['PredecessorIDs'] = df['Predecessors'].apply(names_to_ids)\n",
    "df['SuccessorIDs']   = df['Successors'].apply(names_to_ids)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Coerce weights to numeric and validate\n",
    "# -----------------------------\n",
    "for col in ['EdgeWeight', 'PartnerWeight']:\n",
    "    df[col] = pd.to_numeric(df.get(col), errors='coerce').fillna(0)\n",
    "\n",
    "# Validate non-negative\n",
    "neg_edge = df.loc[df['EdgeWeight'] < 0, ['ID','Assembly','EdgeWeight']]\n",
    "neg_partner = df.loc[df['PartnerWeight'] < 0, ['ID','Assembly','PartnerWeight']]\n",
    "if not neg_edge.empty or not neg_partner.empty:\n",
    "    raise ValueError(f\"Negative weights detected:\\n{neg_edge}\\n{neg_partner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e44e356-04c4-4595-b67b-add142f70551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cycles detected.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5) Initialize directed graph with source and sink\n",
    "# -----------------------------\n",
    "G = nx.DiGraph()\n",
    "source_id = 0\n",
    "sink_id   = max(assy_to_id.values()) + 1\n",
    "G.add_node(source_id, duration=0.0, description='Source', department='')\n",
    "G.add_node(sink_id,   duration=0.0, description='Sink',   department='')\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Add task and partner nodes\n",
    "# -----------------------------\n",
    "task_nodes = set()\n",
    "for _, row in df.iterrows():\n",
    "    tid = int(row['ID'])\n",
    "    task_nodes.add(tid)\n",
    "\n",
    "    dur = float(row['EdgeWeight']) if pd.notna(row['EdgeWeight']) else 0.0\n",
    "    dept = str(row.get('Department', '')).strip()\n",
    "    desc = row.get('Assembly', '')\n",
    "\n",
    "    G.add_node(tid, duration=dur, description=desc, department=dept)\n",
    "\n",
    "    pw = float(row['PartnerWeight']) if pd.notna(row['PartnerWeight']) else 0.0\n",
    "    p  = str(row.get('Partners', '')).strip()\n",
    "    if pw > 0 and p and p.lower() != 'nan':\n",
    "        pid = f\"{p}_{tid}\"  # partner node id (string)\n",
    "        G.add_node(pid, duration=pw, description=f\"{p} for {tid}\", department=dept)\n",
    "        G.add_edge(source_id, pid, weight=0.0)\n",
    "        G.add_edge(pid, tid,      weight=pw)\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Add predecessor edges\n",
    "# -----------------------------\n",
    "for _, row in df.iterrows():\n",
    "    u = int(row['ID'])\n",
    "    for v in row['PredecessorIDs']:\n",
    "        w = float(df.loc[df['ID'] == v, 'EdgeWeight'].iat[0]) if not df.loc[df['ID'] == v, 'EdgeWeight'].empty else 0.0\n",
    "        if not G.has_edge(v, u):\n",
    "            G.add_edge(v, u, weight=w)\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Add successor edges\n",
    "# -----------------------------\n",
    "for _, row in df.iterrows():\n",
    "    u = int(row['ID'])\n",
    "    for v in row['SuccessorIDs']:\n",
    "        w = float(row.get('EdgeWeight', 0.0)) if pd.notna(row.get('EdgeWeight', 0.0)) else 0.0\n",
    "        if not G.has_edge(u, v):\n",
    "            G.add_edge(u, v, weight=w)\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Connect isolated tasks to source/sink\n",
    "# -----------------------------\n",
    "for n in task_nodes:\n",
    "    if G.in_degree(n) == 0:\n",
    "        G.add_edge(source_id, n, weight=0.0)\n",
    "    if G.out_degree(n) == 0:\n",
    "        G.add_edge(n, sink_id, weight=float(G.nodes[n].get('duration', 0.0) or 0.0))\n",
    "\n",
    "# -----------------------------\n",
    "# Check cycles\n",
    "# -----------------------------\n",
    "id_to_assy = {v: k for k, v in assy_to_id.items()}\n",
    "cycles = list(nx.simple_cycles(G))\n",
    "if cycles:\n",
    "    print(\"Warning: cycles detected!\")\n",
    "    for cycle in cycles:\n",
    "        names = [\n",
    "            id_to_assy[node] if isinstance(node, int) and node in id_to_assy else str(node)\n",
    "            for node in cycle\n",
    "        ]\n",
    "        loop = names + [names[0]]\n",
    "        print(\"Cycle: \" + \" -> \".join(loop))\n",
    "else:\n",
    "    print(\"No cycles detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c274c34d-2671-4801-98f4-3e80da6b2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote output to /Users/jimmy/Projects/SunswiftTimeline/python/Results/10_8_25/timeline_062fc6b6.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 10) Compute ES and EF (robust to NaN durations)\n",
    "# -----------------------------\n",
    "def node_dur(n):\n",
    "    d = G.nodes[n].get('duration', 0.0)\n",
    "    return 0.0 if pd.isna(d) else float(d)\n",
    "\n",
    "topo = list(nx.topological_sort(G))\n",
    "ES = {n: 0.0 for n in topo}\n",
    "EF = {}\n",
    "for n in topo:\n",
    "    ES[n] = max((EF[p] for p in G.predecessors(n)), default=0.0)\n",
    "    EF[n] = ES[n] + node_dur(n)\n",
    "\n",
    "# -----------------------------\n",
    "# 11) Compute LS and LF (robust to missing successors)\n",
    "# -----------------------------\n",
    "def safe_min(iterable, default):\n",
    "    vals = [x for x in iterable if pd.notna(x)]\n",
    "    return min(vals) if vals else default\n",
    "\n",
    "LS, LF = {}, {}\n",
    "for n in reversed(topo):\n",
    "    if n == sink_id:\n",
    "        LF[n] = EF[n]\n",
    "    else:\n",
    "        LF[n] = safe_min((LS.get(s) for s in G.successors(n)), default=EF[n])\n",
    "    LS[n] = LF[n] - node_dur(n)\n",
    "\n",
    "# -----------------------------\n",
    "# 12) Helper: format week label (no dates)\n",
    "# -----------------------------\n",
    "def fmt_week(w, base=0, prefix='W'):\n",
    "    return None if pd.isna(w) else f\"{prefix}{int(w + base)}\"\n",
    "\n",
    "# -----------------------------\n",
    "# 13) Build schedule DataFrame\n",
    "# -----------------------------\n",
    "rows = []\n",
    "for n in topo:\n",
    "    rows.append({\n",
    "      'ID':          n,\n",
    "      'Description': G.nodes[n].get('description', ''),\n",
    "      'Department':  G.nodes[n].get('department', ''),\n",
    "      'Duration':    node_dur(n),\n",
    "      'ES':          ES[n],\n",
    "      'ES_Week':     fmt_week(ES[n]),   # week label\n",
    "      'EF':          EF[n],\n",
    "      'EF_Week':     fmt_week(EF[n]),\n",
    "      'LS':          LS[n],\n",
    "      'LS_Week':     fmt_week(LS[n]),\n",
    "      'LF':          LF[n],\n",
    "      'LF_Week':     fmt_week(LF[n]),\n",
    "      'Slack':       LS[n] - ES[n],\n",
    "      'Critical':    (LS[n] - ES[n] == 0)\n",
    "    })\n",
    "res_df = pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 14) Department schedule\n",
    "# -----------------------------\n",
    "task_nodes_int = [n for n in task_nodes if isinstance(n, int)]\n",
    "dept_df = (\n",
    "  res_df[res_df['ID'].isin(task_nodes_int)]\n",
    "    [['Department','ID','Description','ES','ES_Week','EF','EF_Week','LS','LS_Week','Duration']]\n",
    "    .sort_values(['Department','ES','ID'])\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 15) Edge list\n",
    "# -----------------------------\n",
    "edge_data = pd.DataFrame([\n",
    "  {'Edge': f\"{u}->{v}\",\n",
    "   'Description': f\"{G.nodes[u].get('description','')}->{G.nodes[v].get('description','')}\",\n",
    "   'Weight': d.get('weight', 0.0)}\n",
    "  for u, v, d in G.edges(data=True)\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 16) Critical path finder\n",
    "# -----------------------------\n",
    "def find_critical(graph, src, sink):\n",
    "    path = [sink]\n",
    "    cur = sink\n",
    "    while cur != src:\n",
    "        preds = list(graph.predecessors(cur))\n",
    "        if not preds:\n",
    "            break\n",
    "        # Choose predecessor p that satisfies LF[cur] == EF[p] (on critical path)\n",
    "        # If none, pick the one with max EF[p]\n",
    "        cand = [p for p in preds if np.isclose(LF[cur], EF.get(p, -1e18))]\n",
    "        if cand:\n",
    "            cur = max(cand, key=lambda p: EF.get(p, -1e18))\n",
    "        else:\n",
    "            cur = max(preds, key=lambda p: EF.get(p, -1e18))\n",
    "        path.append(cur)\n",
    "    return list(reversed(path))\n",
    "\n",
    "crit = find_critical(G, source_id, sink_id)\n",
    "\n",
    "# -----------------------------\n",
    "# 17) All simple paths (excluding trivial)\n",
    "# -----------------------------\n",
    "allp = [p for p in nx.all_simple_paths(G, source_id, sink_id) if len(p) > 2]\n",
    "\n",
    "# -----------------------------\n",
    "# 18) Paths DataFrame\n",
    "# -----------------------------\n",
    "def mk_paths(paths, critical_path):\n",
    "    out = []\n",
    "    for p in paths:\n",
    "        ids = [n for n in p if n in task_nodes]\n",
    "        desc = [f\"{G.nodes[n]['description']} ({node_dur(n)}w, {fmt_week(ES[n])})\" for n in ids]\n",
    "        total = sum(node_dur(n) for n in ids)\n",
    "        out.append({\n",
    "          'Path_ID': str(uuid.uuid4()),\n",
    "          'Task_IDs': ','.join(map(str, ids)),\n",
    "          'Descriptions': ' -> '.join(desc) if desc else 'No tasks',\n",
    "          'Total_Duration': total,\n",
    "          'Is_Critical': (p == critical_path)\n",
    "        })\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "paths_df     = mk_paths([crit], crit)\n",
    "all_paths_df = mk_paths(allp,  crit)\n",
    "\n",
    "# -----------------------------\n",
    "# 19) Export to Excel\n",
    "# -----------------------------\n",
    "with pd.ExcelWriter(outname, engine='xlsxwriter') as w:\n",
    "    df.to_excel(w,           sheet_name='Raw_Tasks',          index=False)\n",
    "    edge_data.to_excel(w,    sheet_name='Edges',              index=False)\n",
    "    res_df.to_excel(w,       sheet_name='Scheduled_Results',  index=False)\n",
    "    paths_df.to_excel(w,     sheet_name='Critical_Path',      index=False)\n",
    "    all_paths_df.to_excel(w, sheet_name='All_Paths',          index=False)\n",
    "    dept_df.to_excel(w,      sheet_name='Department_Schedule',index=False)\n",
    "\n",
    "print(f\"Wrote output to {outname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20e4f659-352f-4244-9511-74c93be9ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) Gantt charts\n",
    "gantt_dir = os.path.join(os.path.dirname(outname), \"gantt_charts\")\n",
    "shutil.rmtree(gantt_dir, ignore_errors=True)\n",
    "os.makedirs(gantt_dir, exist_ok=True)\n",
    "\n",
    "def create_gantt(path_nodes, df_in, color, fname, title):\n",
    "    # keep only real task IDs (exclude source/sink and partner string nodes)\n",
    "    ids = [n for n in path_nodes if isinstance(n, int) and n in task_nodes]\n",
    "    sub = df_in[df_in['ID'].isin(ids)].sort_values('ES')\n",
    "    if sub.empty:\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(1.0, len(sub)*0.5 + 1)))\n",
    "    for _, r in sub.iterrows():\n",
    "        # Week labels instead of dates\n",
    "        lbl = f\"{r['Description']}\\n{r['ES_Week']} to {r['EF_Week']}\"\n",
    "        ax.barh(lbl, r['Duration'], left=r['ES'], color=color, edgecolor='black')\n",
    "\n",
    "    ax.set_xlabel('Weeks')\n",
    "    ax.set_title(title)\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    pathfile = os.path.join(gantt_dir, fname)\n",
    "    plt.savefig(pathfile, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Critical path Gantt\n",
    "crit_duration = sum(\n",
    "    G.nodes[n]['duration'] for n in crit\n",
    "    if isinstance(n, int) and n not in (source_id, sink_id)\n",
    ")\n",
    "create_gantt(\n",
    "    crit,\n",
    "    res_df,\n",
    "    'red',\n",
    "    'critical_path.png',\n",
    "    f\"Critical Path ({crit_duration}w)\"\n",
    ")\n",
    "\n",
    "# Department Gantts\n",
    "for dept in dept_df['Department'].dropna().unique():\n",
    "    if not str(dept).strip():\n",
    "        continue\n",
    "    subs = dept_df[dept_df['Department'] == dept]\n",
    "    create_gantt(\n",
    "        subs['ID'].tolist(),\n",
    "        subs,\n",
    "        'blue',\n",
    "        f\"dept_{str(dept).lower().replace(' ', '_')}.png\",\n",
    "        f\"{dept} Department\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0edec-2248-499a-98c9-fe3618526a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SunswiftTimeline (venv)",
   "language": "python",
   "name": "sunswift-tl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
